{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cda22ec",
   "metadata": {},
   "source": [
    "sub-BH243.nwb\n",
    "2 days ago1.09 MB\n",
    "sub-BH243_ses-20211103_obj-1dppqzr.nwb\n",
    "2 days ago1.09 MB\n",
    "sub-BH243_ses-20211103_obj-1okdjl.nwb\n",
    "2 days ago1.04 MB\n",
    "sub-BH243_ses-20211103_obj-hmtn3m.nwb\n",
    "2 days ago1.04 MB\n",
    "sub-BH243_ses-20211103_obj-zp74wq.nwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eb04f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available processing modules: dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "# Define the file path\n",
    "input_file_path = 'data\\sub-BH243_ses-20211103_obj-1okdjl.nwb'\n",
    "\n",
    "# Read the NWB file\n",
    "with NWBHDF5IO(input_file_path, 'r') as io:\n",
    "    nwbfile = io.read()\n",
    "    \n",
    "    # List the top-level data groups\n",
    "    print(\"Available processing modules:\", nwbfile.processing.keys())\n",
    "    \n",
    "    # If \"ecephys\" is not available, print other data interfaces\n",
    "    for module_name, module in nwbfile.processing.items():\n",
    "        print(f\"Module: {module_name}\")\n",
    "        for name, data_interface in module.data_interfaces.items():\n",
    "            print(f\"  Data Interface: {name} - {type(data_interface)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f863a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available acquisition data:\n",
      "\n",
      "Other available data interfaces:\n",
      "  Analysis: {}\n",
      "  Stimulus: {}\n",
      "  Scratch: {}\n"
     ]
    }
   ],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "# Define the file path\n",
    "input_file_path = 'data\\sub-BH243_ses-20211103_obj-1okdjl.nwb'\n",
    "\n",
    "# Read the NWB file\n",
    "with NWBHDF5IO(input_file_path, 'r') as io:\n",
    "    nwbfile = io.read()\n",
    "\n",
    "    # Explore the acquisition module\n",
    "    print(\"Available acquisition data:\")\n",
    "    for name, data_interface in nwbfile.acquisition.items():\n",
    "        print(f\"  Name: {name}, Type: {type(data_interface)}\")\n",
    "    \n",
    "    # Explore other potential sections\n",
    "    print(\"\\nOther available data interfaces:\")\n",
    "    print(\"  Analysis:\", nwbfile.analysis)\n",
    "    print(\"  Stimulus:\", nwbfile.stimulus)\n",
    "    print(\"  Scratch:\", nwbfile.scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10aefc95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: /acquisition\n",
      "Group: /analysis\n",
      "Dataset: /file_create_date - Shape: (1,), Dtype: object\n",
      "Group: /general\n",
      "Group: /general/devices\n",
      "Group: /general/extracellular_ephys\n",
      "Dataset: /general/institution - Shape: (), Dtype: object\n",
      "Group: /general/intracellular_ephys\n",
      "Group: /general/optogenetics\n",
      "Group: /general/optophysiology\n",
      "Group: /general/subject\n",
      "Dataset: /general/subject/age - Shape: (), Dtype: object\n",
      "Dataset: /general/subject/description - Shape: (), Dtype: object\n",
      "Dataset: /general/subject/sex - Shape: (), Dtype: object\n",
      "Dataset: /general/subject/species - Shape: (), Dtype: object\n",
      "Dataset: /general/subject/subject_id - Shape: (), Dtype: object\n",
      "Dataset: /identifier - Shape: (), Dtype: object\n",
      "Group: /intervals\n",
      "Group: /processing\n",
      "Group: /scratch\n",
      "Dataset: /session_description - Shape: (), Dtype: object\n",
      "Dataset: /session_start_time - Shape: (), Dtype: object\n",
      "Group: /specifications\n",
      "Group: /specifications/core\n",
      "Group: /specifications/core/2.4.0\n",
      "Dataset: /specifications/core/2.4.0/namespace - Shape: (), Dtype: object\n",
      "Dataset: /specifications/core/2.4.0/nwb.base - Shape: (), Dtype: object\n",
      "Dataset: /specifications/core/2.4.0/nwb.behavior - Shape: (), Dtype: object\n",
      "Dataset: /specifications/core/2.4.0/nwb.device - Shape: (), Dtype: object\n",
      "Dataset: /specifications/core/2.4.0/nwb.ecephys - Shape: (), Dtype: object\n",
      "Dataset: /specifications/core/2.4.0/nwb.epoch - Shape: (), Dtype: object\n",
      "Dataset: /specifications/core/2.4.0/nwb.file - Shape: (), Dtype: object\n",
      "Dataset: /specifications/core/2.4.0/nwb.icephys - Shape: (), Dtype: object\n",
      "Dataset: /specifications/core/2.4.0/nwb.image - Shape: (), Dtype: object\n",
      "Dataset: /specifications/core/2.4.0/nwb.misc - Shape: (), Dtype: object\n",
      "Dataset: /specifications/core/2.4.0/nwb.ogen - Shape: (), Dtype: object\n",
      "Dataset: /specifications/core/2.4.0/nwb.ophys - Shape: (), Dtype: object\n",
      "Dataset: /specifications/core/2.4.0/nwb.retinotopy - Shape: (), Dtype: object\n",
      "Group: /specifications/hdmf-common\n",
      "Group: /specifications/hdmf-common/1.5.0\n",
      "Dataset: /specifications/hdmf-common/1.5.0/base - Shape: (), Dtype: object\n",
      "Dataset: /specifications/hdmf-common/1.5.0/namespace - Shape: (), Dtype: object\n",
      "Dataset: /specifications/hdmf-common/1.5.0/sparse - Shape: (), Dtype: object\n",
      "Dataset: /specifications/hdmf-common/1.5.0/table - Shape: (), Dtype: object\n",
      "Group: /specifications/hdmf-experimental\n",
      "Group: /specifications/hdmf-experimental/0.1.0\n",
      "Dataset: /specifications/hdmf-experimental/0.1.0/experimental - Shape: (), Dtype: object\n",
      "Dataset: /specifications/hdmf-experimental/0.1.0/namespace - Shape: (), Dtype: object\n",
      "Dataset: /specifications/hdmf-experimental/0.1.0/resources - Shape: (), Dtype: object\n",
      "Group: /stimulus\n",
      "Group: /stimulus/presentation\n",
      "Group: /stimulus/templates\n",
      "Dataset: /timestamps_reference_time - Shape: (), Dtype: object\n",
      "Group: /units\n",
      "Dataset: /units/id - Shape: (20,), Dtype: int64\n",
      "Dataset: /units/spike_times - Shape: (111477,), Dtype: float64\n",
      "Dataset: /units/spike_times_index - Shape: (20,), Dtype: uint64\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Define the file path\n",
    "input_file_path = 'data\\sub-BH243_ses-20211103_obj-1okdjl.nwb'\n",
    "\n",
    "# Open the NWB file as an HDF5 file and recursively list its contents\n",
    "def explore_hdf5_group(group, path='/'):\n",
    "    for key in group.keys():\n",
    "        item = group[key]\n",
    "        if isinstance(item, h5py.Group):\n",
    "            print(f\"Group: {path}{key}\")\n",
    "            explore_hdf5_group(item, path=path + key + '/')\n",
    "        elif isinstance(item, h5py.Dataset):\n",
    "            print(f\"Dataset: {path}{key} - Shape: {item.shape}, Dtype: {item.dtype}\")\n",
    "\n",
    "with h5py.File(input_file_path, 'r') as f:\n",
    "    explore_hdf5_group(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1355032",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "NWB file does not contain ecephys data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m     channel_names \u001b[38;5;241m=\u001b[39m data_interface\u001b[38;5;241m.\u001b[39melectrodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m][:]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNWB file does not contain ecephys data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Prepare EDF file writer\u001b[39;00m\n\u001b[0;32m     24\u001b[0m n_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(channel_names)\n",
      "\u001b[1;31mValueError\u001b[0m: NWB file does not contain ecephys data."
     ]
    }
   ],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "import pyedflib\n",
    "import numpy as np\n",
    "\n",
    "# Define the file paths\n",
    "input_file_path = 'data\\sub-BH243_ses-20211103_obj-1okdjl.nwb'  # Update with your NWB file path\n",
    "output_file_path = 'data\\sub-BH243_ses-20211103_obj-1okdjl.edf'\n",
    "\n",
    "# Read the NWB file\n",
    "with NWBHDF5IO(input_file_path, 'r') as io:\n",
    "    nwbfile = io.read()\n",
    "    \n",
    "    # Extract data and metadata (example assumes a single time series)\n",
    "    if 'ecephys' in nwbfile.processing:\n",
    "        ecephys = nwbfile.processing['ecephys']\n",
    "        data_interface = next(iter(ecephys.data_interfaces.values()))\n",
    "        data = data_interface.data[:]\n",
    "        timestamps = data_interface.timestamps[:]\n",
    "        channel_names = data_interface.electrodes['label'][:]\n",
    "    else:\n",
    "        raise ValueError(\"NWB file does not contain ecephys data.\")\n",
    "\n",
    "    # Prepare EDF file writer\n",
    "    n_channels = len(channel_names)\n",
    "    signal_headers = []\n",
    "    signal_data = []\n",
    "\n",
    "    for i, channel in enumerate(channel_names):\n",
    "        signal_headers.append({\n",
    "            'label': channel,\n",
    "            'dimension': 'uV',\n",
    "            'sample_rate': 1 / np.mean(np.diff(timestamps)),\n",
    "            'physical_max': np.max(data[:, i]),\n",
    "            'physical_min': np.min(data[:, i]),\n",
    "            'digital_max': 32767,\n",
    "            'digital_min': -32768,\n",
    "            'transducer': '',\n",
    "            'prefilter': ''\n",
    "        })\n",
    "        # Scale data to fit EDF limits\n",
    "        scaled_data = np.int16((data[:, i] - np.min(data[:, i])) /\n",
    "                               (np.max(data[:, i]) - np.min(data[:, i])) * (32767 - (-32768)) + (-32768))\n",
    "        signal_data.append(scaled_data)\n",
    "\n",
    "    # Write to EDF file\n",
    "    with pyedflib.EdfWriter(output_file_path, n_channels=n_channels, file_type=pyedflib.FILETYPE_EDFPLUS) as edf:\n",
    "        edf.setSignalHeaders(signal_headers)\n",
    "        edf.writeSamples(signal_data)\n",
    "\n",
    "print(\"Conversion to EDF format completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dbc2574",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'flags'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pyedflib\u001b[38;5;241m.\u001b[39mEdfWriter(output_file_path, n_channels\u001b[38;5;241m=\u001b[39mnum_units, file_type\u001b[38;5;241m=\u001b[39mpyedflib\u001b[38;5;241m.\u001b[39mFILETYPE_EDFPLUS) \u001b[38;5;28;01mas\u001b[39;00m edf:\n\u001b[0;32m     44\u001b[0m     edf\u001b[38;5;241m.\u001b[39msetSignalHeaders(signal_headers)\n\u001b[1;32m---> 45\u001b[0m     \u001b[43medf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteSamples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinary_signals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEDF file with binary spike data created successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\ipytorch\\lib\\site-packages\\pyedflib\\edfwriter.py:843\u001b[0m, in \u001b[0;36mEdfWriter.writeSamples\u001b[1;34m(self, data_list, digital)\u001b[0m\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WrongInputSize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of channels (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;124m     unequal to length of data (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels), \u001b[38;5;28mlen\u001b[39m(data_list)))\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# Check for F-contiguous arrays\u001b[39;00m\n\u001b[1;32m--> 843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_contiguous\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    844\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignals are in Fortran order. Will automatically \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    845\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransfer to C order for compatibility with edflib.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    846\u001b[0m     data_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(data_list)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\ipytorch\\lib\\site-packages\\pyedflib\\edfwriter.py:843\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WrongInputSize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of channels (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;124m     unequal to length of data (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels), \u001b[38;5;28mlen\u001b[39m(data_list)))\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# Check for F-contiguous arrays\u001b[39;00m\n\u001b[1;32m--> 843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m data_list):\n\u001b[0;32m    844\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignals are in Fortran order. Will automatically \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    845\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransfer to C order for compatibility with edflib.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    846\u001b[0m     data_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(data_list)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'flags'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyedflib\n",
    "\n",
    "# Define parameters for binary signal representation\n",
    "sampling_frequency = 1000  # 1 kHz sampling frequency\n",
    "output_file_path = 'sub-BH243_spike_binary.edf'\n",
    "\n",
    "# Example spike data (replace with real data)\n",
    "spike_times = np.random.uniform(0, 10, 117397)  # Simulated spike times (in seconds)\n",
    "spike_times_index = np.arange(20) * 5870  # Simulated spike indices for each unit\n",
    "num_units = len(spike_times_index)  # Number of units\n",
    "total_time = spike_times.max()  # Total duration of the recording in seconds\n",
    "\n",
    "# Create a time axis for the binary signals\n",
    "num_samples = int(total_time * sampling_frequency)\n",
    "binary_signals = np.zeros((num_units, num_samples), dtype=np.int16)\n",
    "\n",
    "# Populate binary signals with spikes\n",
    "for unit_idx in range(num_units):\n",
    "    start_idx = spike_times_index[unit_idx]\n",
    "    end_idx = spike_times_index[unit_idx + 1] if unit_idx + 1 < num_units else len(spike_times)\n",
    "    unit_spikes = spike_times[start_idx:end_idx]\n",
    "    spike_indices = (unit_spikes * sampling_frequency).astype(int)\n",
    "    spike_indices = np.clip(spike_indices, 0, num_samples - 1)  # Clip indices to valid range\n",
    "    binary_signals[unit_idx, spike_indices] = 1\n",
    "\n",
    "# Create EDF signal headers\n",
    "signal_headers = []\n",
    "for unit_idx in range(num_units):\n",
    "    signal_headers.append({\n",
    "        'label': f'Unit {unit_idx}',\n",
    "        'dimension': '',\n",
    "        'sample_rate': sampling_frequency,\n",
    "        'physical_max': 1,\n",
    "        'physical_min': 0,\n",
    "        'digital_max': 1,\n",
    "        'digital_min': 0,\n",
    "        'transducer': '',\n",
    "        'prefilter': ''\n",
    "    })\n",
    "\n",
    "# Write binary spike data to EDF file\n",
    "with pyedflib.EdfWriter(output_file_path, n_channels=num_units, file_type=pyedflib.FILETYPE_EDFPLUS) as edf:\n",
    "    edf.setSignalHeaders(signal_headers)\n",
    "    edf.writeSamples(binary_signals.tolist())\n",
    "\n",
    "print(\"EDF file with binary spike data created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d2261a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDF file with binary spike data created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write binary spike data to EDF file\n",
    "with pyedflib.EdfWriter(output_file_path, n_channels=num_units, file_type=pyedflib.FILETYPE_EDFPLUS) as edf:\n",
    "    edf.setSignalHeaders(signal_headers)\n",
    "    edf.writeSamples(binary_signals)  # Use the NumPy array directly\n",
    "\n",
    "print(\"EDF file with binary spike data created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e68aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ipytorch] *",
   "language": "python",
   "name": "conda-env-ipytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
